{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dropout_rate = 0.3\n",
    "    hidden_units = 768\n",
    "    num_heads = 8\n",
    "    device = \"cpu\"\n",
    "    maxlen = 512\n",
    "    num_blocks = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(torch.nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
    "        self.dropout1 = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
    "        self.dropout2 = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.dropout2(self.conv2(self.relu(self.dropout1(self.conv1(inputs.transpose(-1, -2))))))\n",
    "        outputs = outputs.transpose(-1, -2) # as Conv1D requires (N, C, Length)\n",
    "        outputs += inputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(torch.nn.Module):\n",
    "    def __init__(self, user_num, item_num, args):\n",
    "        super(SASRec, self).__init__()\n",
    "\n",
    "        self.user_num = user_num\n",
    "        self.item_num = item_num\n",
    "        self.dev = args.device\n",
    "\n",
    "        # TODO: loss += args.l2_emb for regularizing embedding vectors during training\n",
    "        # https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
    "        self.item_emb = torch.nn.Embedding(self.item_num+1, args.hidden_units, padding_idx=0)\n",
    "        self.pos_emb = torch.nn.Embedding(args.maxlen, args.hidden_units) # TO IMPROVE\n",
    "        self.emb_dropout = torch.nn.Dropout(p=args.dropout_rate)\n",
    "\n",
    "        self.attention_layernorms = torch.nn.ModuleList() # to be Q for self-attention\n",
    "        self.attention_layers = torch.nn.ModuleList()\n",
    "        self.forward_layernorms = torch.nn.ModuleList()\n",
    "        self.forward_layers = torch.nn.ModuleList()\n",
    "\n",
    "        self.last_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
    "\n",
    "        for _ in range(args.num_blocks):\n",
    "            new_attn_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
    "            self.attention_layernorms.append(new_attn_layernorm)\n",
    "\n",
    "            new_attn_layer =  torch.nn.MultiheadAttention(args.hidden_units,\n",
    "                                                            args.num_heads,\n",
    "                                                            args.dropout_rate)\n",
    "            self.attention_layers.append(new_attn_layer)\n",
    "\n",
    "            new_fwd_layernorm = torch.nn.LayerNorm(args.hidden_units, eps=1e-8)\n",
    "            self.forward_layernorms.append(new_fwd_layernorm)\n",
    "\n",
    "            new_fwd_layer = PointWiseFeedForward(args.hidden_units, args.dropout_rate)\n",
    "            self.forward_layers.append(new_fwd_layer)\n",
    "\n",
    "            # self.pos_sigmoid = torch.nn.Sigmoid()\n",
    "            # self.neg_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def log2feats(self, log_seqs):\n",
    "        seqs = self.item_emb(torch.LongTensor(log_seqs).to(self.dev))\n",
    "        seqs *= self.item_emb.embedding_dim ** 0.5\n",
    "        #positions = np.tile(np.array(range(log_seqs.shape[1])), [log_seqs.shape[0], 1])\n",
    "        #seqs += self.pos_emb(torch.LongTensor(positions).to(self.dev))\n",
    "        seqs = self.emb_dropout(seqs)\n",
    "\n",
    "        timeline_mask = torch.BoolTensor(log_seqs == 0).to(self.dev)\n",
    "        seqs *= ~timeline_mask.unsqueeze(-1) # broadcast in last dim\n",
    "\n",
    "        tl = seqs.shape[1] # time dim len for enforce causality\n",
    "        attention_mask = ~torch.tril(torch.ones((tl, tl), dtype=torch.bool, device=self.dev))\n",
    "\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "            Q = self.attention_layernorms[i](seqs)\n",
    "            mha_outputs, _ = self.attention_layers[i](Q, seqs, seqs, \n",
    "                                            attn_mask=attention_mask)\n",
    "                                            # key_padding_mask=timeline_mask\n",
    "                                            # need_weights=False) this arg do not work?\n",
    "            seqs = Q + mha_outputs\n",
    "            seqs = torch.transpose(seqs, 0, 1)\n",
    "\n",
    "            seqs = self.forward_layernorms[i](seqs)\n",
    "            seqs = self.forward_layers[i](seqs)\n",
    "            seqs *=  ~timeline_mask.unsqueeze(-1)\n",
    "\n",
    "        log_feats = self.last_layernorm(seqs) # (U, T, C) -> (U, -1, C)\n",
    "\n",
    "        return log_feats\n",
    "\n",
    "    def forward(self, log_seqs, pos_seqs, neg_seqs): # for training        \n",
    "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
    "        print(\"hi\")\n",
    "        print(\"log feats shape:{}\".format(log_feats.shape))\n",
    "        pos_embs = self.item_emb(torch.LongTensor(pos_seqs).to(self.dev))\n",
    "        neg_embs = self.item_emb(torch.LongTensor(neg_seqs).to(self.dev))\n",
    "\n",
    "        pos_logits = (log_feats * pos_embs).sum(dim=-1)\n",
    "        neg_logits = (log_feats * neg_embs).sum(dim=-1)\n",
    "\n",
    "        # pos_pred = self.pos_sigmoid(pos_logits)\n",
    "        # neg_pred = self.neg_sigmoid(neg_logits)\n",
    "\n",
    "        return pos_logits, neg_logits # pos_pred, neg_pred\n",
    "\n",
    "    def predict(self, log_seqs, item_indices): # for inference\n",
    "        log_feats = self.log2feats(log_seqs) # user_ids hasn't been used yet\n",
    "\n",
    "        final_feat = log_feats[:, -1, :] # only use last QKV classifier, a waste\n",
    "\n",
    "        item_embs = self.item_emb(torch.LongTensor(item_indices).to(self.dev)) # (U, I, C)\n",
    "\n",
    "        logits = item_embs.matmul(final_feat.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # preds = self.pos_sigmoid(logits) # rank same item list for different users\n",
    "\n",
    "        return logits # preds # (U, I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_seq_ids = torch.tensor([[1,12,0,43,45,66]])\n",
    "user_ids = torch.tensor([[0,3,4,5,6,7]])\n",
    "pos_seq = torch.tensor([[12,23,55,67,84,98]])\n",
    "neg_seq = torch.tensor([[23,43,25,17,54,78]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "model = SASRec(1000,1000,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "log feats shape:torch.Size([1, 6, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.1265, -4.0618,  0.0000,  8.8231, 26.9071, 21.7874]],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor([[ 0.4874,  1.5811,  0.0000,  6.9174, 19.3078, 19.2773]],\n",
       "        grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(log_seq_ids,pos_seq,neg_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.RWKV.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWKV_Config:\n",
    "    vocab_size = 768\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    ctx_len = 768\n",
    "    model_type = \"RWKV\"\n",
    "    n_head = 8\n",
    "    n_attn = 8\n",
    "    n_ffn = 4\n",
    "    rwkv_emb_scale = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768   768   4    tok_emb.weight\n",
      "8     768   0    blocks.0.attn.key.weight\n",
      "8     768   1    blocks.0.attn.value.weight\n",
      "8     768   0    blocks.0.attn.receptance.weight\n",
      "768   8     0    blocks.0.attn.output.weight\n",
      "10    768   1    blocks.0.mlp.key.weight\n",
      "10    768   1    blocks.0.mlp.value.weight\n",
      "768   10    0    blocks.0.mlp.weight.weight\n",
      "768   768   0    blocks.0.mlp.receptance.weight\n",
      "8     768   0    blocks.1.attn.key.weight\n",
      "8     768   1    blocks.1.attn.value.weight\n",
      "8     768   0    blocks.1.attn.receptance.weight\n",
      "768   8     0    blocks.1.attn.output.weight\n",
      "10    768   1    blocks.1.mlp.key.weight\n",
      "10    768   1    blocks.1.mlp.value.weight\n",
      "768   10    0    blocks.1.mlp.weight.weight\n",
      "768   768   0    blocks.1.mlp.receptance.weight\n",
      "8     768   0    blocks.2.attn.key.weight\n",
      "8     768   1    blocks.2.attn.value.weight\n",
      "8     768   0    blocks.2.attn.receptance.weight\n",
      "768   8     0    blocks.2.attn.output.weight\n",
      "10    768   1    blocks.2.mlp.key.weight\n",
      "10    768   1    blocks.2.mlp.value.weight\n",
      "768   10    0    blocks.2.mlp.weight.weight\n",
      "768   768   0    blocks.2.mlp.receptance.weight\n",
      "8     768   0    blocks.3.attn.key.weight\n",
      "8     768   1    blocks.3.attn.value.weight\n",
      "8     768   0    blocks.3.attn.receptance.weight\n",
      "768   8     0    blocks.3.attn.output.weight\n",
      "10    768   1    blocks.3.mlp.key.weight\n",
      "10    768   1    blocks.3.mlp.value.weight\n",
      "768   10    0    blocks.3.mlp.weight.weight\n",
      "768   768   0    blocks.3.mlp.receptance.weight\n",
      "8     768   0    blocks.4.attn.key.weight\n",
      "8     768   1    blocks.4.attn.value.weight\n",
      "8     768   0    blocks.4.attn.receptance.weight\n",
      "768   8     0    blocks.4.attn.output.weight\n",
      "10    768   1    blocks.4.mlp.key.weight\n",
      "10    768   1    blocks.4.mlp.value.weight\n",
      "768   10    0    blocks.4.mlp.weight.weight\n",
      "768   768   0    blocks.4.mlp.receptance.weight\n",
      "8     768   0    blocks.5.attn.key.weight\n",
      "8     768   1    blocks.5.attn.value.weight\n",
      "8     768   0    blocks.5.attn.receptance.weight\n",
      "768   8     0    blocks.5.attn.output.weight\n",
      "10    768   1    blocks.5.mlp.key.weight\n",
      "10    768   1    blocks.5.mlp.value.weight\n",
      "768   10    0    blocks.5.mlp.weight.weight\n",
      "768   768   0    blocks.5.mlp.receptance.weight\n",
      "8     768   0    blocks.6.attn.key.weight\n",
      "8     768   1    blocks.6.attn.value.weight\n",
      "8     768   0    blocks.6.attn.receptance.weight\n",
      "768   8     0    blocks.6.attn.output.weight\n",
      "10    768   1    blocks.6.mlp.key.weight\n",
      "10    768   1    blocks.6.mlp.value.weight\n",
      "768   10    0    blocks.6.mlp.weight.weight\n",
      "768   768   0    blocks.6.mlp.receptance.weight\n",
      "8     768   0    blocks.7.attn.key.weight\n",
      "8     768   1    blocks.7.attn.value.weight\n",
      "8     768   0    blocks.7.attn.receptance.weight\n",
      "768   8     0    blocks.7.attn.output.weight\n",
      "10    768   1    blocks.7.mlp.key.weight\n",
      "10    768   1    blocks.7.mlp.value.weight\n",
      "768   10    0    blocks.7.mlp.weight.weight\n",
      "768   768   0    blocks.7.mlp.receptance.weight\n",
      "8     768   0    blocks.8.attn.key.weight\n",
      "8     768   1    blocks.8.attn.value.weight\n",
      "8     768   0    blocks.8.attn.receptance.weight\n",
      "768   8     0    blocks.8.attn.output.weight\n",
      "10    768   1    blocks.8.mlp.key.weight\n",
      "10    768   1    blocks.8.mlp.value.weight\n",
      "768   10    0    blocks.8.mlp.weight.weight\n",
      "768   768   0    blocks.8.mlp.receptance.weight\n",
      "8     768   0    blocks.9.attn.key.weight\n",
      "8     768   1    blocks.9.attn.value.weight\n",
      "8     768   0    blocks.9.attn.receptance.weight\n",
      "768   8     0    blocks.9.attn.output.weight\n",
      "10    768   1    blocks.9.mlp.key.weight\n",
      "10    768   1    blocks.9.mlp.value.weight\n",
      "768   10    0    blocks.9.mlp.weight.weight\n",
      "768   768   0    blocks.9.mlp.receptance.weight\n",
      "8     768   0    blocks.10.attn.key.weight\n",
      "8     768   1    blocks.10.attn.value.weight\n",
      "8     768   0    blocks.10.attn.receptance.weight\n",
      "768   8     0    blocks.10.attn.output.weight\n",
      "10    768   1    blocks.10.mlp.key.weight\n",
      "10    768   1    blocks.10.mlp.value.weight\n",
      "768   10    0    blocks.10.mlp.weight.weight\n",
      "768   768   0    blocks.10.mlp.receptance.weight\n",
      "8     768   0    blocks.11.attn.key.weight\n",
      "8     768   1    blocks.11.attn.value.weight\n",
      "8     768   0    blocks.11.attn.receptance.weight\n",
      "768   8     0    blocks.11.attn.output.weight\n",
      "10    768   1    blocks.11.mlp.key.weight\n",
      "10    768   1    blocks.11.mlp.value.weight\n",
      "768   10    0    blocks.11.mlp.weight.weight\n",
      "768   768   0    blocks.11.mlp.receptance.weight\n",
      "768   768   4    head.weight\n",
      "256   768   0.01 head_q.weight\n",
      "256   768   0.01 head_k.weight\n"
     ]
    }
   ],
   "source": [
    "rwkv = GPT(config = RWKV_Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape :torch.Size([1, 6, 6]).\n",
      "x shape:torch.Size([1, 6, 768])\n",
      "pos embedding shape:torch.Size([1, 6, 768])\n",
      "neg embedding shape:torch.Size([1, 6, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwkv(log_seq_ids,pos_seq,neg_seq)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
